# Review Protocol for Systematic Literature Review on Using LLMs for Programming Assessment

## 1. Introduction
- **Topic**: This systematic review will examine the use of Large Language Models (LLMs) like GPT-3, GPT-4, CodeBERT, and others in evaluating programming assignments in educational settings. The review will focus on understanding the effectiveness, limitations, and potential of LLMs in automating the assessment of student programming work.
- **Review Objective**: The goal of this review is to synthesize existing literature to evaluate the following questions:
  - How effective are LLMs in assessing the correctness, style, and complexity of student programming assignments?
  - What are the challenges, biases, and limitations associated with using LLMs for programming assessment?
  - How do LLM-based systems compare to traditional grading methods (e.g., human grading, static test cases)?
  - What impact do LLM-based evaluations have on student learning, engagement, and feedback?

## 2. Research Questions
- **Primary Question**: How can Large Language Models be effectively used for evaluating student programming assignments?
- **Secondary Questions**:
  1. What are the strengths and weaknesses of LLM-based programming assessment systems compared to traditional methods?
  2. How do LLMs handle common issues in programming assessment, such as code complexity, edge cases, and partial solutions?
  3. What impact do LLM evaluations have on student learning and engagement?
  4. Are there any ethical, fairness, or bias concerns when using LLMs in educational assessment?

## 3. Inclusion and Exclusion Criteria
- **Inclusion Criteria**:
  - Studies that involve LLMs (e.g., GPT-3, GPT-4, CodeBERT, etc.) applied to programming assignment grading, feedback, or evaluation.
  - Studies published in peer-reviewed journals, conferences, or credible academic sources.
  - Papers that provide experimental data or case studies on LLM-based programming assessments.
  - Studies that compare LLM-based grading to traditional methods, or studies discussing the effectiveness, strengths, or limitations of LLM-based grading.
  - Articles published between 2018 and the present (the introduction of GPT-3 in 2020 sparked widespread interest in LLMs for education).

- **Exclusion Criteria**:
  - Studies not specifically focused on programming assessments (e.g., studies that apply LLMs to text-based assignments or non-educational contexts).
  - Articles not peer-reviewed (e.g., conference abstracts, opinion pieces, non-academic blogs).
  - Papers that discuss LLMs for grading but do not involve programming assignments specifically.
  - Studies that do not provide sufficient data or methodological detail.

## 4. Information Sources and Search Strategy
- **Databases**:
  - Google Scholar
  - IEEE Xplore
  - ACM Digital Library
  - SpringerLink
  - PubMed (for health-related aspects of educational assessments)
  - ArXiv (preprints for the latest research)

- **Keywords**:
  - "Large Language Model", "GPT-3", "GPT-4", "CodeBERT"
  - "Programming assessment", "Automated grading", "Code evaluation", "Programming feedback"
  - "AI in education", "LLM for grading", "AI-based assessment in programming"
  - Boolean search strategy example:
    - **("Large Language Model" OR GPT-3 OR GPT-4 OR CodeBERT) AND ("programming assessment" OR "automated grading" OR "code evaluation") AND ("education" OR "learning")**

- **Additional Search Strategy**:
  - Reference lists of relevant articles will be manually checked to identify additional sources.
  - Studies will also be sought through key authors or institutions known for research in LLMs and AI in education.

## 5. Study Selection Process
- **Initial Screening**: 
  - The initial screening will be based on titles and abstracts. Studies that appear relevant will be retrieved in full.
  - Two reviewers will independently conduct this process. Discrepancies will be resolved through discussion or by consulting a third reviewer.

- **Full-Text Review**: 
  - Studies that pass the initial screening will undergo a detailed full-text review for eligibility against the inclusion/exclusion criteria.

- **Data Extraction**: 
  - A standardized extraction form will be used to collect information from each included study, including:
    - Study title, authors, and year of publication.
    - Objectives of the study.
    - Type of programming assignments assessed.
    - Methods used (e.g., LLM type, evaluation process).
    - Key findings (effectiveness of LLM, student feedback, comparison with traditional methods).
    - Quality of the study (e.g., sample size, validity).

## 6. Data Extraction
- **Extraction Form**: A standard form will be developed to extract key information:
  - **Study Information**: Title, authors, year of publication.
  - **Study Objectives**: What was the study trying to evaluate or demonstrate?
  - **Methodology**:
    - Type of LLM used (GPT-3, GPT-4, CodeBERT, etc.).
    - Type of programming tasks (e.g., debugging, coding challenges, full project assessments).
    - Evaluation criteria (e.g., correctness, style, performance).
  - **Findings**:
    - Key outcomes (e.g., accuracy of LLM in assessing student code, student engagement, comparison with human grading).
    - Limitations identified by the authors.
  - **Study Quality**: Type of research (empirical study, theoretical, case study), sample size, and methodological rigor.

## 7. Assessment of Study Quality
- **Quality Assessment Criteria**:
  - **Study Design**: Was the study experimental, observational, or based on simulations? Was it well-controlled?
  - **Sample Size**: How many participants or data points were included in the study?
  - **Bias and Validity**: Did the authors discuss limitations, such as sample bias or generalizability?
  - **Relevance**: Does the study directly address LLMs in the context of programming assessment?

- A scoring system will be developed to rate the quality of each study based on these criteria, and studies with higher quality will be weighted more in the synthesis.

## 8. Data Synthesis and Analysis
- **Qualitative Synthesis**: 
  - The findings from the included studies will be synthesized thematically, focusing on the effectiveness of LLMs in programming assessments, their limitations, student engagement, and potential biases.
  
- **Quantitative Analysis (if applicable)**: 
  - If meta-analysis is feasible (i.e., studies report similar outcomes on LLM performance), the data will be aggregated and analyzed using statistical techniques to determine overall trends (e.g., accuracy of LLM grading).

- **Comparison with Traditional Methods**: 
  - The effectiveness of LLM-based grading will be compared with traditional grading methods, such as human grading and automated tests, in terms of accuracy, reliability, and time efficiency.

## 9. Report Writing
- The review will be structured as follows:
  1. **Introduction**: Context of the study, importance of programming assessments, and the rise of LLMs.
  2. **Methodology**: Detailed explanation of the systematic review process, including search strategy, inclusion/exclusion criteria, data extraction methods, and quality assessment.
  3. **Results**: Summary of the findings from the selected studies, including strengths, limitations, and comparisons.
  4. **Discussion**: Interpretation of results, implications for practice, challenges with LLMs, and recommendations for future research.
  5. **Conclusion**: Final summary of the state of the field, with suggestions for how LLMs can be improved for programming assessment.

---

### **Conclusion**
This protocol outlines a systematic approach to reviewing the use of LLMs in programming assessment. By following these steps, we aim to provide a comprehensive and unbiased evaluation of the current state of research, offering insights into how LLMs can shape future educational practices and assessment strategies.
